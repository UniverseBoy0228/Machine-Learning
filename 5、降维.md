# 降维

## 1、过拟合

> $抑制过拟合方法 \left \{ \begin{array}{lr**} 增加数据\\ 正则化\\ 降维 \left \{ \begin{array}{lr**} 直接降维：特征选择\\ 线性降维：PCA,MDS\\ 非线性降维：Isomap,LLE \end{array} \right. \end{array} \right.$
>
> 维度过高$\longrightarrow$维度灾难$\longrightarrow$数据稀疏性
>
> 维度灾难的几何解释：
>
> ​         <img src="F:/笔记/picture/维度灾难.png" alt="维度灾难" style="zoom: 80%;" /> 
>
> 假设图中为D维空间的超球体，半径为1，绿色部分为一个球壳，球壳厚度为$\epsilon,\epsilon \in (0,1)$
> $$
> \begin{align*}
> & V_{超球体} = k r^D = k\\
> & V_{球壳} = V_{超球体} - V_{内超球体} = k - k (1-\epsilon)^D \\
> & \Longrightarrow \lim_{D\rightarrow \infty} \dfrac{V_{球壳}}{V_{超球体}} = \dfrac{k - k (1-\epsilon)^D}{k} = 1-(1-\epsilon)^D = 1
> \end{align*}
> $$
> 在高维空间中，无论$\epsilon$如何小，球壳面积与超球体面积相等，因此高维空间是分布不均匀的，样本点大部分情况会分布于球壳，而不是球内，这就造成了维度灾难。

## 2、预备知识

> 样本数据：$\{(x_i,y_i)\}_{i=1}^N,x_i\in \R^p$
>
> $X=(x_1,x_2,...,x_N)^T = \left( \begin{array}{lr**} x_1^T \\ x_2^T \\ \vdots \\ x_N^T \end{array} \right) = \left( \matrix{ x_{11}\quad x_{12}\quad \cdots\quad x_{1p} \\ x_{21}\quad x_{22}\quad \cdots\quad x_{2p} \\ \vdots\quad\quad \vdots\quad\quad\quad\quad \vdots \\ x_{N1}\quad x_{N2}\quad \cdots\quad x_{Np} } \right)_{N*p}$
>
> 样本均值：$\textcolor{red}{\overline{X_{p*1}}} = \dfrac{1}{N} \sum\limits_{i=1}^N x_i = \dfrac{1}{N} (x_1\ x_2\ ...\ x_N) \left( \matrix{1 \\ 1 \\ \vdots \\ 1} \right)_{N*1} = \textcolor{red}{\dfrac{1}{N} X^T 1_N} \quad 1_N=\left( \matrix{1 \\ 1 \\ \vdots \\ 1} \right)_{N*1}$
>
> 样本方差：$\textcolor{red}{\overline{S_{p*p}}} = \dfrac{1}{N} \sum\limits_{i=1}^N (x_i-\overline{X})(x_i-\overline{X})^T = \dfrac{1}{N} (x_1-\overline{X}\ x_2-\overline{X}\ ...\ x_N-\overline{X})\left( \matrix{(x_1-\overline{X})^T \\ (x_2-\overline{X})^T \\ \vdots \\ (x_N-\overline{X})^T} \right) \\ = \dfrac{1}{N} [(x_1\ x_2\ ...\ x_N)-(\overline{X}\ \overline{X}\ ...\ \overline{X})] \left( \matrix{(x_1-\overline{X})^T \\ (x_2-\overline{X})^T \\ \vdots \\ (x_N-\overline{X})^T} \right) = \dfrac{1}{N} [X^T-\overline{X} 1_N^T] \left( \matrix{(x_1-\overline{X})^T \\ (x_2-\overline{X})^T \\ \vdots \\ (x_N-\overline{X})^T} \right) \\ =\dfrac{1}{N} [X^T(I_N-\dfrac{1}{N}1_N 1_N^T)][(I_N-\dfrac{1}{N}1_N 1_N^T)^T X] = \dfrac{1}{N} X^THH^TX = \textcolor{red}{\dfrac{1}{N} X^THX}$
>
> 其中：$H = I_N-\dfrac{1}{N}1_N 1_N^T$被称为中心矩阵$(centering\ matrix)，H^T=H，H^n=HH^T=H$。

## 3、PCA

> <img src="F:\笔记\picture\PCA降维.jpg" alt="PCA降维" style="zoom:80%;" />
>
> 一个中心：原始特征空间的重构（相关$\longrightarrow$无关）
>
> 两个基本点：$\left \{ \begin{array}{lr**} 最大投影方差\\ 最小重构代价 \end{array} \right.$
>
> 从几何角度看，即找到图中的$u_1$
>
> ---
>
> ## 3.1 最大投影方差
>
> 样本$x_i$在$u_1$上的投影：$(x_i-\overline{x})u_1$，$x_i-\overline{x}$是为了中心化。
> $$
> \begin{align*}
> 最大投影方差 &= \dfrac{1}{N} \sum\limits_{i=1}^N [(x_i-\overline{x})^Tu_1]^2 \\
> &= \dfrac{1}{N} \sum\limits_{i=1}^N [(x_i-\overline{x})^Tu_1]^T[(x_i-\overline{x})^Tu_1] \\
> &= \dfrac{1}{N} \sum\limits_{i=1}^N u_1^T(x_i-\overline{x})(x_i-\overline{x})^Tu_1 \\
> &= u_1^T [\dfrac{1}{N} \sum\limits_{i=1}^N(x_i-\overline{x})(x_i-\overline{x})^T] u_1 \\
> &= u_1^TSu_1
> \end{align*}
> $$
> 优化目标：$\left \{ \begin{array}{lr**} \hat{u}_1 = arg \max u_1^TSu_1 \\ s.t.\quad u_1^Tu_1=1 \end{array} \right.$
>
> 拉格朗日乘子法：
> $$
> \begin{align*}
> & L(u_1) = u_1^TSu_1 + \lambda (1-u_1^Tu_1) \\
> & \dfrac{\partial L(u_1)}{\partial u_1} = 2Su_1 - \lambda 2u_1 = 0 \\
> & \Longrightarrow Su_1=\lambda u_1
> \end{align*}
> $$
> 最终可得：$\lambda$为特征值，$u_1$为特征向量。
>
> ## 3.2 最小重构代价
>
> $u_k$是$S$的特征向量，因此样本数据$x_i$可由$u_k$线性表示：$x_i = \sum\limits_{i=1}^p ((x_i-\overline{x})^Tu_k)u_k$
>
> $\textcolor{red}{取p维中的前q维进行降维，即用u_1,u_2,...,u_q表示x_i:\hat{x_i} = \sum\limits_{i=1}^q ((x_i-\overline{x})^Tu_k)u_k}$
>
> 最小重构代价即为：
> $$
> \begin{align*}
> J &= \dfrac{1}{N} \sum\limits_{i=1}^N \Vert x_i-\hat{x_i} \Vert ^2 \\
> &= \dfrac{1}{N} \sum\limits_{i=1}^N \{ \sum\limits_{k=q+1}^p \Vert [(x_i-\overline{x})^Tu_k]u_k \Vert ^2 \} \\
> &= \dfrac{1}{N} \sum\limits_{i=1}^N \sum\limits_{k=q+1}^p [(x_i-\overline{x})^Tu_k]^2 \Vert u_k \Vert ^2 \\
> &= \dfrac{1}{N} \sum\limits_{i=1}^N \sum\limits_{k=q+1}^p [(x_i-\overline{x})^Tu_k]^2 \\
> &= \sum\limits_{k=q+1}^p \{ \dfrac{1}{N} \sum\limits_{i=1}^N [(x_i-\overline{x})^Tu_k]^2 \} \\
> &= \sum\limits_{k=q+1}^p (u_k^TSu_k)
> \end{align*}
> $$
> 优化目标：
>
> $\left \{ \begin{array}{lr**} \hat{u}_k = arg \min \sum\limits_{k=q+1}^p u_k^TSu_k \\ s.t.\quad u_k^Tu_k=1 \end{array} \right.$
>
> 由此可得，应该取p维中特征值最大的前q维特征向量进行降维。
>
> 
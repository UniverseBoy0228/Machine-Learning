# 概率图模型

## 1、背景介绍

> 高维随机变量$P(x_1,x_2,...,x_p) \left \{ \begin{array}{lr**} 边缘概率:P(x_i) \\ 条件概率:P(x_i\vert x_j) \end{array} \right.$
>
> 四条规则：$\left \{ \begin{array}{lr**} Sum\ rule:P(x_1)=\int P(x_1,x_2)\mathrm{d}x_2 \\ Product\ rule:P(x_1,x_2)=P(x_1)P(x_2\vert x_1)=P(x_2)P(x_1\vert x_2) \\ Chain\ rule:P(x_1,x_2,...,x_p)=\prod\limits_{i=1}^p P(x_i\vert x_1,x_2,...,x_{i-1}) \\ Bayesian\ rule:P(x_2\vert x_1)=\dfrac{P(x_1,x_2)}{P(x_1)}=\dfrac{P(x_1\vert x_2)P(x_2)}{\int P(x_1,x_2)\mathrm{d}x_2}=\dfrac{P(x_1\vert x_2)P(x_2)}{\int P(x_1\vert x_2)P(x_2)\mathrm{d}x_2} \end{array} \right.$
>
> 概率图：$\left \{ \begin{array}{lr**} Representation:表示 \left \{ \begin{array}{lr**} 有向图：Bayesian\ Network \\ 高斯图(连续) \left \{ \begin{array}{lr**} Gaussian\ BN \\ Gaussian\ MN \end{array} \right. \\ 无向图：Markov\ Network \end{array} \right. \\ Inference:推断 \left \{ \begin{array}{lr**} 精确推断 \\ 近似推断 \left \{ \begin{array}{lr**} 确定性近似(变分推断) \\ 随机近似(MCMC) \end{array} \right. \end{array} \right. \\ Learning:学习 \left \{ \begin{array}{lr**} 参数学习 \\ 结构学习 \end{array} \right. \end{array} \right.$

## 2、贝叶斯网络(Bayesian Network)

> 无条件独立假设：$P(x_1,x_2,...,x_p)=P(x_1)\prod\limits_{i=2}^p P(x_i\vert x_1,x_2,...,x_{i-1})$
>
> 贝叶斯网络的条件独立性假设：$x_A \perp x_C \vert x_B$
>
> 贝叶斯网络的因子分解：$P(x_1,x_2,...,x_p)=\prod\limits_{i=1}^p P(x_i\vert P_{pa(x_i)}) \quad pa(x_i)表示x_i的父节点集合$
>
> ---
>
> <img src="F:\笔记\picture\贝叶斯网络.png" alt="贝叶斯网络" style="zoom: 80%;" />
> $$
> P(A,B,C,D,E,F,G)=P(A)P(B\vert A)P(C\vert A)P(D)P(E\vert C,D)P(F\vert E)P(G\vert B,F)
> $$
>
> ---
>
> ### 2.1 D-separation(有向分离)：
>
> (1) tail-to-tail
>
> ![tail-to-tail](F:\笔记\picture\tail-to-tail.png)
>
> 
> $$
> a\perp b \vert c
> $$
>
> $$
> 若c被观测，则路径被阻塞
> $$
>
> (2) head-to-tail
>
> ![head-to-tail](F:\笔记\picture\head-to-tail.png)
> $$
> a\perp b \vert c
> $$
>
> $$
> 若c被观测，则路径被阻塞
> $$
>
> (3) head-to-head
>
> ![head-to-head](F:\笔记\picture\head-to-head.png)
> $$
> 默认情况下，a\vert b，路径是阻塞的；若c被观测，则路径是通的
> $$
>
> ### 2.2 Markov Blanket(马尔科夫毯)
>
> $$
> \begin{align*}
> P(x_i\vert x_{-i}) &= \dfrac{P(x_i,x_{-i})}{P(x_{-i})} = \dfrac{P(x)}{\int_{x_i} P(x) \mathrm{d}x_i} \\
> &= \dfrac{\prod\limits_{i=1}^p P(x_j\vert x_{par(j)})}{\int_{x_i} \prod\limits_{i=1}^p P(x_j\vert x_{par(j)}) \mathrm{d}x_i} \\
> &= \dfrac{\Delta * \overline{\Delta}}{\int \Delta * \overline{\Delta} \mathrm{d}x_i} = \dfrac{\Delta * \overline{\Delta}}{\Delta * \int \overline{\Delta} \mathrm{d}x_i} \\
> &= \dfrac{\overline{\Delta}}{\int \overline{\Delta} \mathrm{d}x_i} = f(\overline{\Delta})
> \end{align*}
> $$
>
> $\Delta表示与x_i无关的样本，\overline{\Delta}表示与x_i有关的样本$
>
> $\textcolor{red}{由上述推导可知，x_i与其他所有样本的关系最终都会演变成与x_i相关的样本上。}$

## 3、马尔科夫随机场(Markov Network)

> ### 3.1 马尔科夫随机场的条件独立性
>
> 主要表现在如下三个方面：
>
> (1) 全局Markov
>
> $x_A \perp x_C \vert x_B$
>
> (2) 局部Markov
>
> $a \perp \{ 全集-a-邻居 \} \vert \{ 邻居 \}$
>
> (3) 成对Markov
>
> $x_i \perp x_j \vert x_{-i-j} \quad (i \ne j)$
>
> 上述三个方面的关系：$(1)\Longleftrightarrow(2)\Longleftrightarrow(3)$
>
> ### 3.2 因子分解
>
> 团：一个关于结点的集合，集合的结点之间相互连通。
>
> $\textcolor{red}{马尔科夫随机场的概率分布表示方法：基于最大团的因子分解}$
>
> $$
> P(x)=\dfrac{1}{Z} \prod\limits_{i=1}^k \psi(x_{c_i})
> $$
>
> $$
> Z=\sum\limits_x \prod\limits_{i=1}^k \psi(x_{c_i})=\sum\limits_{x_1} \sum\limits_{x_2} ... \sum\limits_{x_p} \prod\limits_{i=1}^p \psi(x_{c_i})
> $$
>
> 
>
> $c_i表示最大团，x_{c_i}表示最大团随机变量集合，\psi(x_{c_i})表示势函数，必须为正，Z为归一化因子$
>
> $基于最大团的因子分解\Longleftrightarrow^{Hammersley-Clifford定理}(1)\Longleftrightarrow(2)\Longleftrightarrow(3)$
>
> 因子分解中的势函数一般都为能量函数：
> $$
> \psi(x_{c_i})=\exp \{ -E[x_{c_i}] \}
> $$
> 由能量函数定义而成的分布$P(x)$称为$Gibbs\ Distribution(吉布斯分布)或Boltzmann\ Distribution(玻尔兹曼分布)$
> $$
> \begin{align*}
> P(x) &= \dfrac{1}{Z} \prod\limits_{i=1}^k \psi(x_{c_i}) \\
> &= \dfrac{1}{Z} \prod\limits_{i=1}^k \exp \{ -E[x_{c_i}] \} \\
> &= \underbrace{\dfrac{1}{Z} \exp \{ - \sum\limits_{i=1}^k E[x_{c_i}] \}} \quad 指数族分布
> \end{align*}
> $$
> $\textcolor{red}{最大熵原理\Longrightarrow 指数族分布(Gibbs\ Distribution)}$
>
> 因此可知：$\textcolor{red}{Markov\ Random\ Field \Longleftrightarrow Gibbs\ Distribution}$

## 4、Inference推断

> $P(x) = P(x_1,x_2,...,x_p)$
>
> $\left \{ \begin{array}{lr**} 边缘概率：P(x_i) = \sum\limits_{x_1} \cdots \sum\limits_{x_{i-1}} \sum\limits_{x_{i+1}} \cdots \sum\limits_{x_p} P(x) \\ 条件概率：P(x_A \vert x_B) \quad x=x_A\cup x_B \\ MAP\ Inference：\hat{z} = arg \max_z P(z\vert x) \propto arg \max P(z,x) \end{array} \right.$
>
> $Inference \left \{ \begin{array}{lr**} 精确推断 \left \{ \begin{array}{lr**} Variable\ Elimination(VE,变量消除推断) \\ Belief\ Propagation(BP,Sum-Product\ Algorithm,信念传播推断) \\ Junction\ Tree\ Algorithm(联合树算法) \end{array} \right. \\ 近似推断 \left \{ \begin{array}{lr**} Loop\ Belief\ Inference \\ Mente\ Carlo\ Inference(蒙特卡罗推断):Importance\ Sampling,MCMC \\ Variational\ Inference(变分推断) \end{array} \right. \end{array} \right.$

## 5、Variable Elimination

> ![]()
>
> 假设$a, b,c, d$是离散的二值随机变量，$a, b, c, d \in \{ 0, 1 \}$
> $$
> \begin{align*}
> P(d) &= \sum\limits_{a,b,c} P(a, b, c, d) \\
> &= \sum\limits_{a,b,c} P(a)P(b\vert a)P(c\vert b)P(d\vert c)
> \end{align*}
> $$
>
> 常规求解：
> $$
> \begin{align*}
> ① &= P(a=0)P(b=0\vert a=0)P(c=0\vert b=0)P(d\vert c=0) \\
> &+ P(a=1)P(b=0\vert a=1)P(c=0\vert b=0)P(d\vert c=0) \\
> &+ \cdots \\
> &+ P(a=1)P(b=1\vert a=1)P(c=1\vert b=1)P(d\vert c=1) \\
> &= 8个因子乘积的和
> \end{align*}
> $$
>
> 变量消除法求解：
> $$
> \begin{align*}
> ② &= \sum\limits_{b, c} P(c\vert b)P(d\vert c) \sum\limits_a P(a)P(b\vert a) \\
> &= \sum\limits_{b, c} P(c\vert b)P(d\vert c) \phi_a(b) \\
> &= \sum\limits_{c} P(d\vert c) \sum\limits_{b} P(c\vert b)\phi_a(b) \\
> &= \sum\limits_{c} P(c\vert b) \phi_b(c) \\
> &= \phi_c(d)
> \end{align*}
> $$

## 6、Belief Propagation

> BP在VE的基础上解决了重复计算的问题
>
> ![]()

## 7、Max Product

> ① BP的改进
>
> ②Viterbi的推广

## 8、Moral Graph道德图

> 产生的原因：想把有向图转化成无向图
>
> ![]()

## 9、Factor Graph因子图

> 道德图：有向图（树）$\Longrightarrow$ 无向图（引入环）
>
> 因子图：$x=x_1,x_2,...,x_p,P(x)=\prod\limits_s f_s(x_s),s$是图的节点集合，$x_s$是$x$的随机变量子集
>
> ![]()


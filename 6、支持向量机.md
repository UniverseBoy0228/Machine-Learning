# 支持向量机

![支持向量机]()

SVM三个核心：间隔、对偶、核技巧

$\left \{ \begin{array}{lr**} hard-margin\ SVM:硬间隔 \\ soft-margin\ SVM:软间隔 \\ kernel\ SVM \end{array}  \right.$

判别模型：$f(w)=sign(w^T+b)$

## 1、硬间隔SVM（最大间隔分类器）

> 样本数据：$\{ (x_i,y_i) \}_{i=1}^N,y_i\in \{ -1, 1\}$
>
> 优化目标：最大间隔
> $$
> \begin{align*}
> & \max margin(w,b) \\
> & s.t. \quad \left \{ \begin{array}{lr**} w^tx_i+b>0,y_i=1 \\ w^tx_i+b<0,y_i=0 \end{array} \right. \quad i=1,2,...,N \quad \Longrightarrow y_i(w^Tx_i+b) > 0
> \end{align*}
> $$
>
> $$
> margin(w,b) = \min\limits_{w,b,x_i,i=1,2,...,N} distance(w,b,x_i) = \min\limits_{w,b,x_i,i=1,2,...,N} \dfrac{1}{\Vert w \Vert}\vert w^Tx_i+b \vert
> $$
>
> 最终优化目标化简为：
> $$
> \left \{ \begin{array}{lr**} \max\limits_{w,b} \min\limits_{x_i,i=1,2,...,N} \dfrac{1}{\Vert w \Vert}\vert w^Tx_i+b \vert \quad ① \\ s.t. \quad y_i(w^Tx_i+b) > 0,i=1,2,...,N \quad ② \end{array} \right.
> $$
>
> ---
>
> 一系列化简操作：
>
> $y_i(w^Tx_i+b) = \vert y_i(w^Tx_i+b) \vert = \vert y_i \vert \vert (w^Tx_i+b) \vert = \vert (w^Tx_i+b) \vert$
>
> $① \Longrightarrow \max\limits_{w,b} \dfrac{1}{\Vert w \Vert} \min\limits_{x_i,i=1,2,...,N} y_i(w^Tx_i+b)$
>
> $② \Longrightarrow \exist \gamma > 0,使\min\limits_{x_i,i=1,2,...,N} y_i(w^Tx_i+b) = \gamma \quad 因此\Longrightarrow y_i(w^Tx_i+b) > \gamma$
>
> $因此① \Longrightarrow \max\limits_{w,b} \dfrac{1}{\Vert w \Vert}\gamma$
>
> ---
>
> 化简后的优化目标：
> $$
> \left \{ \begin{array}{lr**} \max\limits_{w,b} \dfrac{1}{\Vert w \Vert}\gamma \\ s.t. \quad y_i(w^Tx_i+b) \ge \gamma,i=1,2,...,N \end{array} \right. \quad \gamma > 0
> $$
> $\gamma$为常数，不影响优化，这里令$\gamma = 1$
>
> 因此：
> $$
> \left \{ \begin{array}{lr**} \max\limits_{w,b} \dfrac{1}{\Vert w \Vert} \Longrightarrow \min\limits_{w,b} \Vert w \Vert \Longrightarrow \min\limits_{w,b} \dfrac{1}{2}\Vert w \Vert \\ s.t. \quad y_i(w^Tx_i+b) \ge 1,i=1,2,...,N \end{array} \right.
> $$
> 最终得到：
> $$
> \left \{ \begin{array}{lr**} \min\limits_{w,b} \dfrac{1}{2}w^Tw \\ s.t. \quad y_i(w^Tx_i+b) \ge 1,i=1,2,...,N \end{array} \right.
> $$
> $\textcolor{red}{上面的优化问题中，对w,b有约束，很难求解，因此需要转化为易于求解的形式}$
> $$
> \left \{ \begin{array}{lr**} \min\limits_{w,b} \dfrac{1}{2}w^Tw \\ s.t. \quad y_i(w^Tx_i+b) \ge 1,i=1,2,...,N \end{array} \right. \Longrightarrow \left \{ \begin{array}{lr**} \min\limits_{w,b} \max\limits_{\lambda} \dfrac{1}{2} w^Tw + \sum\limits_{i=1}^N \lambda_i [1-y_i(w^Tx_i+b)] \\ s.t. \quad \lambda_i \ge 0 \end{array} \right.
> $$
> $\textcolor{red}{转化为对w,b无约束的优化问题}$
>
> ---
>
> 证明：
>
> 拉格朗日乘数法：
> $$
> \begin{align*}
> & L(w,b,\lambda) = \dfrac{1}{2} w^Tw + \sum\limits_{i=1}^N \lambda_i [1-y_i(w^Tx_i+b)] \\
> & \lambda_i \ge 0,i=1,2,...,N \\
> & 令\Delta = 1-y_i(w^Tx_i+b) \\
> & ① 当\Delta > 0 时 \\
> & \max\limits_\lambda L(w,b,\lambda) = \dfrac{1}{2} w^Tw + \sum\limits_{i=1}^N \lambda_i [1-y_i(w^Tx_i+b)] = \dfrac{1}{2} w^Tw + \infty \quad \lambda_i取\infty \\
> & ② 当\Delta \le 0 时 \\
> & \max\limits_\lambda L(w,b,\lambda) = \dfrac{1}{2} w^Tw + \sum\limits_{i=1}^N \lambda_i [1-y_i(w^Tx_i+b)] = \dfrac{1}{2} w^Tw + 0 \quad \lambda_i取0 \\
> & \Longrightarrow \min\limits_{w,b} \max\limits_{\lambda} L(w,b,\lambda) = \min\limits_{w,b}(\infty, \dfrac{1}{2} w^Tw) = \min\limits_{w,b} \dfrac{1}{2} w^Tw
> \end{align*}
> $$
>
> ---
>
> $\textcolor{red}{将上面的优化问题转化为其对偶问题(SVM是二次规划问题，满足强对偶关系)}$
> $$
> \left \{ \begin{array}{lr**} \max\limits_{\lambda} \min\limits_{w,b} \dfrac{1}{2} w^Tw + \sum\limits_{i=1}^N \lambda_i [1-y_i(w^Tx_i+b)] \\ s.t. \quad \lambda_i \ge 0 \end{array} \right.
> $$
> 求解$\min\limits_{w,b} \dfrac{1}{2} w^Tw + \sum\limits_{i=1}^N \lambda_i [1-y_i(w^Tx_i+b)]$
> $$
> \begin{align*}
> & \dfrac{\partial L}{\partial b} = \dfrac{\partial}{\partial b} [-\sum\limits_{i=1}^N \lambda_iy_ib] = -\sum\limits_{i=1}^N \lambda_iy_i = 0 \\
> & \Longrightarrow \sum\limits_{i=1}^N \lambda_iy_i = 0
> \end{align*}
> $$
> 将$\sum\limits_{i=1}^N \lambda_iy_i = 0$代入$L(w,b,\lambda)$
> $$
> \begin{align*}
> L(w,b,\lambda) &= \dfrac{1}{2} w^Tw + \sum\limits_{i=1}^N \lambda_i [1-y_i(w^Tx_i+b)] \\
> &= \dfrac{1}{2} w^Tw + \sum\limits_{i=1}^N \lambda_i - \sum\limits_{i=1}^N \lambda_iy_iw^Tx_i - \sum\limits_{i=1}^N \lambda_iy_ib \\
> &= \dfrac{1}{2} w^Tw + \sum\limits_{i=1}^N \lambda_i - \sum\limits_{i=1}^N \lambda_iy_iw^Tx_i
> \end{align*}
> $$
>
> $$
> \begin{align*}
> & \dfrac{\partial L}{\partial w} = \dfrac{1}{2}*2w-\sum\limits_{i=1}^N \lambda_iy_ix_i = 0 \\
> & \Longrightarrow w^* = \sum\limits_{i=1}^N \lambda_iy_ix_i
> \end{align*}
> $$
>
> 将$w^* = \sum\limits_{i=1}^N \lambda_iy_ix_i$代入$L(w,b,\lambda)$
> $$
> \begin{align*}
> \min\limits_{w,b} L(w,b,w) &= \dfrac{1}{2} (\sum\limits_{i=1}^N \lambda_iy_ix_i)^T(\sum\limits_{i=1}^N \lambda_iy_ix_i) - \sum\limits_{i=1}^N \lambda_iy_i(\sum\limits_{i=1}^N \lambda_iy_ix_i)x_i + \sum\limits_{i=1}^N \lambda_i \\ 
> &= \dfrac{1}{2} \sum\limits_{i=1}^N \sum\limits_{j=1}^N \lambda_i\lambda_jy_iy_jx_i^Tx_j - \sum\limits_{i=1}^N \sum\limits_{j=1}^N \lambda_i\lambda_jy_iy_jx_i^Tx_j + \sum\limits_{i=1}^N \lambda_i \\
> &= -\dfrac{1}{2} \sum\limits_{i=1}^N \sum\limits_{j=1}^N \lambda_i\lambda_jy_iy_jx_i^Tx_j + \sum\limits_{i=1}^N \lambda_i
> \end{align*}
> $$
> 最终得到化简的优化目标：
> $$
> \left \{ \begin{array}{lr**} \min\limits_{\lambda} \dfrac{1}{2} \sum\limits_{i=1}^N \sum\limits_{j=1}^N \lambda_i\lambda_jy_iy_jx_i^Tx_j - \sum\limits_{i=1}^N \lambda_i \\ s.t. \quad \lambda_i \ge 0 , \sum\limits_{i=1}^N \lambda_iy_i = 0 \end{array} \right.
> $$
>
> ---
>
> $\textcolor{red}{原、对偶问题满足强对偶关系\Longleftrightarrow满足KKT条件}$
>
> KKT条件：
>
> $\left \{ \begin{array}{lr**} \dfrac{\partial L}{\partial w}=0,\dfrac{\partial L}{\partial b}=0 \\ \lambda_i(1-y_i(w^Tx_i+b))=0 \quad 松弛互补条件 \\ \lambda_i \ge 0 \\ 1-y_i(w^Tx_i+b) \le 0 \end{array} \right.$
>
> ---
>
> $\exist (x_k,y_k),使1-y_k(w^Tx_k+b) = 0$
> $$
> \begin{align*}
> & y_k(w^Tx_k+b) = 1 \\
> & y_k^2(w^Tx_k+b) = y_k \\
> & w^Tx_k+b = y_k \quad y_k^2 = 1 \\
> & \Longrightarrow b^*=y_k-w^Tx_k = y_k-\sum\limits_{i=1}^N \lambda_iy_ix_i^Tx_k \\
> \\
> & 因此\left \{ \begin{array}{lr**} w^*=\sum\limits_{i=1}^N \lambda_iy_ix_i \\ b^*=y_k-\sum\limits_{i=1}^N \lambda_iy_ix_i^Tx_k \end{array} \right. \\
> & \Longrightarrow f(w)=sing(w^*x+b^*)
> \end{align*}
> $$

## 2、软间隔SVM

> 硬间隔SVM的前提是理想状态数据必可分，但现实情况不会如此理想，数据可能不能完全可分。
>
> 软间隔SVM是允许少量错误。
>
> $\min \dfrac{1}{2} w^Tw+loss$
>
> 可得：
> $$
> \left \{ \begin{array}{lr**} \min\limits_{w,b} \dfrac{1}{2}w^Tw+C\sum\limits_{i=1}^N \xi_i,C是常数 \\ s.t. \quad y_i(w^Tx_i+b) \ge 1-\xi_i,\xi_i \ge 0,i=1,2,...,N \end{array} \right.
> $$

## 3、核方法

> | 线性可分        | 有一点点错误     | 严格非线性 |
> | --------------- | ---------------- | ---------- |
> | 感知机PLA       | Pocket Algorithm | Kernel PLA |
> | Hard-margin SVM | Soft-margin SVM  | Kernel SVM |
>
> $\textcolor{red}{非线性可分\longrightarrow \phi(x) \longrightarrow 线性可分,\phi(x)为非线性转换}$
>
> 复杂的非线性变换函数一般很难寻找到，因此使用到了核技巧
>
> $\left \{ \begin{array}{lr**} 非线性带来高维转换:x\longrightarrow \phi(x) \\ 对偶表示带来内积:x_i^Tx_j \longrightarrow \phi(x_i)^T\phi(x_j) \end{array} \right.$
>
> $\textcolor{red}{通过核技巧直接计算\phi(x_i)^T\phi(x_j)}$
>
> ---
>
> 核函数：
> $$
> k(x,x') = \phi(x)^T\phi(x')
> $$
> 对于任意的$x,x' \in X,\exist \phi:x\longrightarrow z$,满足$k(x,x') = \phi(x)^T\phi(x')$,则称$k(x,x')$为一个核函数。
>
> 


# 线性回归

## 1、最小二乘法(LSE)

> 样本数据：$D=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\},x_i\in \R^p,y_i\in \R,i=1,2,\cdots,N$
>
> $X=(x_1,x_2,\cdots,x_N)^T=\left( \matrix{x_{11} \quad x_{12} \quad \cdots \quad x_{1N} \\ x_{21} \quad x_{22} \quad \cdots \quad x_{2N} \\ \vdots \quad\quad \vdots \quad\quad\quad\quad\quad \vdots \\ x_{N1} \quad x_{N2} \quad \cdots \quad x_{NN} \\} \right)_{N*p}$
>
> Loss Function：
> $$
> \begin{align*}
> L(w)&=\sum\limits_{i=1}^N \Vert w^Tx_i-y_i\Vert^2=\sum\limits_{i=1}^N (w^Tx_i-y_i)^2 \\
> &=(w^Tx_1-y_1 \quad w^Tx_1-y_1 \quad \cdots \quad w^Tx_1-y_1) \left( \matrix{w^Tx_1-y_1 \\ w^Tx_2-y_2  \\ \vdots \\ w^Tx_N-y_N }\right) \\
> &=[(w^Tx_1 \quad w^Tx_2 \quad \cdots \quad w^Tx_N)-(y_1 \quad y_2 \quad \cdots y_N)] \left( \matrix{w^Tx_1-y_1 \\ w^Tx_2-y_2  \\ \vdots \\ w^Tx_N-y_N }\right) \\
> &=(W^TX^T-Y^T)(XW-Y)=W^TX^TXW-W^TX^TY-Y^TXW+Y^TY \\
> &=W^TX^TXW-2W^TX^TY+Y^TY
> \end{align*}
> $$
>
> $$
> \begin{align*}
> &\dfrac{\partial L(W)}{\partial W}=2X^TXW-2X^TY=0 \\
> &\Longrightarrow X^TXW=X^TY \\
> &\Longrightarrow \hat{M}=(X^TX)^{-1}X^TY
> \end{align*}
> $$
>
> $(X^TX)^{-1}X^T$被称为伪逆。

## 2、概率视角看线性回归

> 样本数据：$D=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\},x_i\in \R^p,y_i\in \R,i=1,2,\cdots,N$
>
> $\epsilon\sim N(0,\sigma^2)$
> $$
> \begin{align*}
> &\left \{ \begin{array}{lr**} y=f(w)+\epsilon \\ f(w)=w^T \end{array} \right. \\ 
> &\Longrightarrow y=w^Tx+\epsilon
> \end{align*}
> $$
>
> 极大似然估计(MLE)：
> $$
> \begin{align*}
> L(w)&=\log P(y\vert x;w)=\log\prod\limits_{i=1}^N P(y_i\vert x_i;w) \\
> &=\log\prod\limits_{i=1}^N \dfrac{1}{\sqrt{2\pi}\sigma}\exp[-\dfrac{(y_i-w^Tx_i)^2}{2\sigma^2}] \\
> &=\sum\limits_{i=1}^N \log\dfrac{1}{\sqrt{2\pi}\sigma}-\sum\limits_{i=1}^N \dfrac{(y_i-w^Tx_i)^2}{2\sigma^2} \\
> &\hat{w}=arg\max\limits_w [-\sum\limits_{i=1}^N \dfrac{(y_i-w^Tx_i)^2}{2\sigma^2}] \\
> &arg\min\limits_w \sum\limits_{i=1}^N (y_i-w^Tx_i)^2
> \end{align*}
> $$
> $\textcolor{red}{因此：LSE\Longleftrightarrow MLE(noise\ is\ Gaussian\ Dist.)}$

## 3、正则化

> 正则化框架：$arg\min\limits_w [L(w)+\lambda P(w)]$
>
> $L1正则：Lasso，1范数，P(w)=\Vert w \Vert_1$
>
> $L2正则：Ridge(岭回归)，2范数，P(w)=\Vert w \Vert_2^2=w^Tw$
>
> ---
>
> ### 3.1 频率角度
>
> $$
> \begin{align*}
> J(w)&=L(w)+\lambda P(w) \\
> &=\sum\limits_{i=1}^N \Vert w^Tx_i-y_i\Vert^2+\lambda\Vert w \Vert_2^2 \\
> &=\sum\limits_{i=1}^N (w^Tx_i-y_i)(w^Tx_i-y_i)^T+\lambda w^Tw \\
> &=(W^TX^T-Y^T)(XW-Y)+\lambda w^Tw \\
> &=W^TX^TXW-2W^TX^TY+Y^TY+\lambda w^Tw \\
> &=W^T(X^TX+\lambda I)W-2W^TX^TY+Y^TY \\
> \end{align*}
> $$
>
> $$
> \begin{align*}
> &\hat{w}=arg\min\limits_w J(w) \\
> &\dfrac{\partial J(w)}{\partial w}=2T(X^TX+\lambda I)W-2X^TY=0 \\
> &\Longrightarrow \hat{w}=(X^TX+\lambda I)^{-1}X^TY
> \end{align*}
> $$
>
> $X^TX不一定可逆，X^TX为半正定，X^TX+\lambda I一定可逆，有效的抑制了过拟合。$
>
> ---
>
> ### 3.2 贝叶斯角度
>
> $w服从先验分布，w\sim N(0,\sigma_0^2)，\epsilon\sim N(0,\sigma^2)$
>
> $P(w\vert y)=\dfrac{P(y\vert w)P(w)}{P(y)}，y=w^Tx+\epsilon$
>
> 最大后验概率(MAP)：
> $$
> \begin{align*}
> &P(w)=\dfrac{1}{\sqrt{2\pi}\sigma_0}\exp[-\dfrac{\Vert w\Vert_2^2}{2\sigma_0^2}] \\
> &(y\vert x;w)\sim N(w^Tx,\sigma^2) \\
> &P(y\vert x;w)=\dfrac{1}{\sqrt{2\pi}\sigma}\exp[-\dfrac{(y-w^Tx)^2}{2\sigma^2}] \\
> \end{align*}
> $$
>
> $$
> \begin{align*}
> \hat{w}&=arg\max\limits_w P(w\vert y)=arg\max\limits_w P(y\vert w)P(w) \\
> &=arg\max\limits_w \log[P(y\vert w)P(w)] \\
> &=\log[\dfrac{1}{\sqrt{2\pi}\sigma_0}\dfrac{1}{\sqrt{2\pi}\sigma} \exp(-\dfrac{(y-w^Tx)^2}{2\sigma^2}-\dfrac{\Vert w\Vert_2^2}{2\sigma_0^2})] \\
> &=arg\max\limits_w (-\dfrac{(y-w^Tx)^2}{2\sigma^2}-\dfrac{\Vert w\Vert_2^2}{2\sigma_0^2}) \\
> &=arg\min\limits_w [(y-w^Tx)^2+\dfrac{\sigma^2}{\sigma_0}\Vert w\Vert_2^2]
> \end{align*}
> $$
>
> 与频率角度的$J(w)$相等，$\textcolor{red}{因此：Regularized\ LSE\Longleftrightarrow MAP(noise\ is\ GD. and prior\ is\ GD.)}$
>
> 


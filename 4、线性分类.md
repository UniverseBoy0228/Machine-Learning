# 线性分类

## 1、背景介绍

> $频率派\longrightarrow统计机器学习\longrightarrow线性回归\longrightarrow线性分类:y=f(w^T+b)\left \{ \begin{array}{lr**} 硬分类:y\in \{0,1\} \left \{ \begin{array}{lr***}  线性判别分析 \\ 感知机 \end{array}\right. \\ 软分类:y\in[0,1] \left \{ \begin{array}{lr***} 生成式 \left \{ \begin{array}{lr***} 朴素贝叶斯 \\ 高斯判别分析 \end{array}\right. \\ 判别式:逻辑回归 \end{array}\right. \end{array} \right.$
>
> $f:激活函数,f^{-1}:链接函数$

## 2、感知机

> <img src="F:\笔记\picture\感知机1.png" alt="感知机" style="zoom: 50%;" /> <img src="F:\笔记\picture\感知机2.png" alt="感知机"  />
>
> 感知器思想：错误驱动
>
> 样本数据：$\{(x_i,y_i)\}_{i=1}^N,x_i\in\R^p$
> $$
> \begin{align*}
> &f(x)=sign(w^Tx) \\ 
> &sign(a)=\left \{ \begin{array}{lr**} +1,a\geq 0 \\ -1,a< 0 \end{array} \right.
> \end{align*}
> $$
> $Loss\ Function：$
> $$
> \begin{align*}
> &L(w)=\sum\limits_{i=1}^N I\{y_iw^Tx_i<0\} \\
> &I:示性函数,I(A)=\left \{ \begin{array}{lr**} 1,A发生 \\ 0,A不发生 \end{array} \right. \\
> \end{align*}
> $$
> 但是示性函数$I(\cdot)$不可导，因此采用如下形式：
> $$
> \begin{align*}
> &L(w)=\sum\limits_{x_i\in D} (-y_iw^Tx_i) \\
> &\dfrac{\partial L(w)}{\partial w}=-y_ix_i \\
> &w^{t + 1}\longleftarrow w^t-\lambda \dfrac{\partial L(w)}{\partial w}=w^t+\lambda y_ix_i
> \end{align*}
> $$

## 3、线性判别分析(fisher)

> <img src="F:\笔记\picture\线性判别分析.jpg" alt="线性判别逆袭" style="zoom: 67%;" />
>
> 线性判别分析思想：类内间距小，类间间距大
>
> 样本数据：$X=(x_1,x_2,...,x_N)^T=\left( \matrix{x_1^T \\ x_2^T \\ \vdots \\ x_N^T} \right)_{N*p},Y=\left( \matrix{y_1 \\ y_2 \\ \vdots \\ y_N} \right)_{N*1},y_i \in \{+1,-1\}$
>
> $x_{c_1} = \{x_i \vert y_i=+1\},x_{c_2} = \{x_i \vert y_i=-1\},\vert x_{c_1} \vert = N_1,\vert x_{c_2}\vert=N_2,N_1+N_2=N$
> $$
> \begin{align*}
> &z_i=w^Tx_i,z_i为x_i在w上的投影 \\
> &\overline{Z}=\dfrac{1}{N}\sum\limits_{i=1}^N z_i=\dfrac{1}{N} \sum\limits_{i=1}^N w^Tx_i \\
> &S_z=\dfrac{1}{N} \sum\limits_{i=1}^N (z_i-\overline{Z})(z_i-\overline{Z})^T \\
> \end{align*}
> $$
>
> $$
> \begin{align*}
> 对于C_1：&\overline{Z_1}=\dfrac{1}{N_1}\sum\limits_{i=1}^{N_1} z_i=\dfrac{1}{N_1} \sum\limits_{i=1}^{N_1} w^Tx_i \\
> &S_1=\dfrac{1}{N_1} \sum\limits_{i=1}^{N_1} (z_i-\overline{Z_1})(z_i-\overline{Z_1})^T \\
> 对于C_2：&\overline{Z_2}=\dfrac{1}{N_2}\sum\limits_{i=1}^{N_2} z_i=\dfrac{1}{N_2} \sum\limits_{i=1}^{N_2} w^Tx_i \\
> &S_2=\dfrac{1}{N_2} \sum\limits_{i=1}^{N_2} (z_i-\overline{Z_2})(z_i-\overline{Z_2})^T \\
> \end{align*}
> $$
>
> 目标函数：
> $$
> \begin{align*}
> &arg\max_w J(w)=\dfrac{(\overline{Z_1}-\overline{Z_2})^2}{S_1+S_2} \\
> &(\overline{Z_1}-\overline{Z_2})^2表示类间距离 \\
> &S_1+S_2表示类内方差
> \end{align*}
> $$
>
> $$
> \begin{align*}
> (\overline{Z_1}-\overline{Z_2})^2&=(\dfrac{1}{N_1}\sum\limits_{i=1}^{N_1} w^Tx_i-\dfrac{1}{N_2}\sum\limits_{i=1}^{N_2} w^Tx_i)(\dfrac{1}{N_1}\sum\limits_{i=1}^{N_1} w^Tx_i-\dfrac{1}{N_2}\sum\limits_{i=1}^{N_2} w^Tx_i)^T \\
> &=w^T(\dfrac{1}{N_1}\sum\limits_{i=1}^{N_1} x_i-\dfrac{1}{N_2}\sum\limits_{i=1}^{N_2} x_i)(\dfrac{1}{N_1}\sum\limits_{i=1}^{N_1} x_i-\dfrac{1}{N_2}\sum\limits_{i=1}^{N_2} x_i)^T w \\
> &=w^T(\overline{x_{c_1}}-\overline{x_{c_2}})(\overline{x_{c_1}}-\overline{x_{c_2}})^Tw
> \end{align*}
> $$
>
> $$
> \begin{align*}
> S_1+S_2&=\dfrac{1}{N_1}\sum\limits_{i=1}^{N_1}(w^Tx_i-\dfrac{1}{N_1}\sum\limits_{j=1}^{N_1}w^Tx_j)(w^Tx_i-\dfrac{1}{N_1}\sum\limits_{j=1}^{N_1}w^Tx_j)^T \\
> &+\dfrac{1}{N_2}\sum\limits_{i=1}^{N_2}(w^Tx_i-\dfrac{1}{N_2}\sum\limits_{j=1}^{N_2}w^Tx_j)(w^Tx_i-\dfrac{1}{N_2}\sum\limits_{j=1}^{N_2}w^Tx_j)^T \\
> &=w^T[\dfrac{1}{N_1}\sum\limits_{i=1}^{N_1}(x_i-\dfrac{1}{N_1}\sum\limits_{j=1}^{N_1}x_j)(x_i-\dfrac{1}{N_1}\sum\limits_{j=1}^{N_1}x_j)^T]w \\
> &+w^T[\dfrac{1}{N_2}\sum\limits_{i=1}^{N_2}(x_i-\dfrac{1}{N_2}\sum\limits_{j=1}^{N_2}x_j)(x_i-\dfrac{1}{N_2}\sum\limits_{j=1}^{N_2} x_j)^T]w \\
> &=w^TS_{c_1}w+w^TS_{c_2}w=w^T(S_{c_1}+S_{c_2})w \\
> &\Longrightarrow J(w)=\dfrac{w^T(\overline{x_{c_1}}-\overline{x_{c_2}})(\overline{x_{c_1}}-\overline{x_{c_2}})^Tw}{w^T(S_{c_1}+S_{c_2})w}
> \end{align*}
> $$
>
> 令$S_b=(\overline{x_{c_1}}-\overline{x_{c_2}})(\overline{x_{c_1}}-\overline{x_{c_2}}),S_w=S_{c_1}+S_{c_2}$
>
> 则$J(w)=\dfrac{w^TS_bw}{w^TS_ww}=(w^TS_bw)(w^TS_ww)^{-1}$
> $$
> \dfrac{\partial J(w)}{\partial w}=2S_b(w^TS_ww)^{-1}+(w^TS_bw)(-1)(w^TS_ww)^{-2} *2 S_w w=0
> $$
> 两边乘以$(w^TS_ww)^2$得
> $$
> \begin{align*}
> &S_bw(w^TS_ww)=(w^TS_bw)S_ww \\
> &\Longrightarrow S_bw=\dfrac{w^TS_bw}{w^TS_ww}S_ww \\
> &\Longrightarrow w\propto S_w^{-1}S_bw=S_w^{-1}(\overline{x_{c_1}}-\overline{x_{c_2}})(\overline{x_{c_1}}-\overline{x_{c_2}})^Tw \\
> &=w \propto S_w^{-1}(\overline{x_{c_1}}-\overline{x_{c_2}})
> \end{align*}
> $$
>
> 若$S_w^{-1}$是对角阵且各向同性，则$S_w^{-1}\propto I$
> $$
> \Longrightarrow w \propto (\overline{x_{c_1}}-\overline{x_{c_2}})
> $$

## 4、逻辑回归

>样本数据：$\{(x_i,y_i)\}_{i=1}^N,x_i\in \R^p,y_i\in \{0,1\}$
>
>激活函数：$Sigmoid\ Function\ \sigma(z)=\dfrac{1}{1+e^{-z}}$
>
>![sigmoid function](F:\笔记\picture\sigmoid.jpg)
>$$
>\begin{align*}
>&\left \{  \begin{array}{lr**} p_1=P(y=1\vert x)=\dfrac{1}{1+e^{-w^Tx}} \\ p_0=P(y=0\vert x)=1-p1=\dfrac{e^{-w^Tx}}{1+e^{-w^Tx}} \end{array} \right. \\
>&\Longrightarrow P(y\vert x)=p_1^yp_0^{1-y}
>\end{align*}
>$$
>极大似然估计(MLE)：
>$$
>\begin{align*}
>\hat{w}&=arg \max_w \log P(y\vert x) \\
>&=arg \max_w \log \prod\limits_{i=1}^N P(y_i\vert x_i) \\
>&=arg \max_w \sum\limits_{i=1}^N [y_i\log p_1+(1-y_i)\log p_0] \\
>&=arg \min_w \sum\limits_{i=1}^N [-y_i\log p_1-(1-y_i)\log p_0]
>\end{align*}
>$$
>其中：$-y_i\log p_1-(1-y_i)\log p_0]$被称为是交叉熵$(cross\ entropy)$。
>
>$\textcolor{red}{\max\  MLE \Longrightarrow \min\ cross\ entropy}$

## 5、高斯判别分析

> 样本数据：$\{(x_i,y_i)\}_{i=1}^N,x_i\in \R^p,y_i\in \{0,1\}$
>
> $N_1:y=1;N_2:y=0;N_1+N_2=N$
>
> 优化目标：$\hat{y}=arg\max_{y} P(y\vert x)=arg\max_{y} P(x\vert y)P(y)=arg\max_{y} P(x,y)$
>
> $假设：y\sim Bernoulli(\phi) \Longrightarrow P(y)=\phi^y(1-\phi)^{1-y} \\ \quad\quad\  x\vert y \sim N(\mu_1,\Sigma),x\vert y \sim N(\mu_2,\Sigma),每个类别都服从GD. \\ \quad\quad\ \Longrightarrow P(x\vert y)=N(\mu_1,\Sigma)^y N(\mu_2,\Sigma)^{1-y}$
>
> 极大既然函数：
> $$
> \begin{align*}
> L(\theta)&=\log \prod\limits_{i=1}^N P(x_i \vert y_i)P(y_i) \\
> &=\sum\limits_{i=1}^N [\log P(x_i \vert y_i) + \log P(y_i)] \\
> &=\sum\limits_{i=1}^N \{ \log [N(\mu_1,\Sigma)^{y_i} N(\mu_2,\Sigma)^{1-y_i}] + \log [\phi^{y_i}(1-\phi)^{1-y_i}] \} \\
> &=\sum\limits_{i=1}^N \{ \log [N(\mu_1,\Sigma)^{y_i}] + \log [N(\mu_2,\Sigma)^{1-y_i}] + \log [\phi^{y_i}(1-\phi)^{1-y_i}] \} \\
> &=\sum\limits_{i=1}^N [① + ② + ③]
> \end{align*}
> $$
>
> ---
>
> 求$\phi$：
> $$
> \begin{align*}
> ③ &= \sum\limits_{i=1}^N \log [\phi^{y_i}(1-\phi)^{1-y_i}] \\
> &= \sum\limits_{i=1}^N [y_i \log \phi + (1-y_i) \log (1-\phi)] \\
> &\dfrac{\partial ③}{\partial \phi} = \sum\limits_{i=1}^N [y_i \dfrac{1}{\phi} - (1-y_i) \dfrac{1}{1-\phi}] = 0 \\
> &\Longrightarrow \hat{\phi} = \dfrac{1}{N} \sum\limits_{i=1}^N y_i = \dfrac{N_1}{N}
> \end{align*}
> $$
>
> ---
>
> 求$\mu_1$：
> $$
> \begin{align*}
> ① &= \sum\limits_{i=1}^N \log [N(\mu_1,\Sigma)^{y_i}] = \sum\limits_{i=1}^N y_i\log N(\mu_1,\Sigma) \\
> &= \sum\limits_{i=1}^N y_i \log \{ \dfrac{1}{(2\pi)^{\dfrac{p}{2}} \vert \Sigma \vert ^{\dfrac{1}{2}}} \exp{[-\dfrac{1}{2} (x_i-\mu_1)^T \Sigma^{-1} (x_i-\mu_1)]} \} \\
> &\Longrightarrow \hat{\mu_1} = arg \max_{\mu_1} \sum\limits_{i=1}^N y_i[-\dfrac{1}{2} (x_i-\mu_1)^T \Sigma^{-1} (x_i-\mu_1)]
> \end{align*}
> $$
>
> $$
> \begin{align*}
> 令\Delta &= -\dfrac{1}{2} \sum\limits_{i=1}^N y_i[(x_i-\mu_1)^T \Sigma^{-1} (x_i-\mu_1)] \\
> &= -\dfrac{1}{2} \sum\limits_{i=1}^N y_i[(x_i^T \Sigma^{-1}-\mu_1^T \Sigma^{-1}) (x_i-\mu_1)] \\
> &= -\dfrac{1}{2} \sum\limits_{i=1}^N y_i(x_i^T\Sigma^{-1}x_i - x_i^T\Sigma^{-1}\mu_1 - \mu_1^T\Sigma^{-1}x_i - \mu_1^T\Sigma^{-1}\mu_1) \\
> &= -\dfrac{1}{2} \sum\limits_{i=1}^N y_i(x_i^T\Sigma^{-1}x_i - 2\mu_1^T\Sigma^{-1}x_i - \mu_1^T\Sigma^{-1}\mu_1)
> \end{align*}
> $$
>
> $$
> \begin{align*}
> &\dfrac{\partial \Delta}{\partial \mu_1} = -\dfrac{1}{2} \sum\limits_{i=1}^N y_i(-2\Sigma^{-1}x_i+2\Sigma^{-1}\mu_1) = 0 \\
> &\Longrightarrow \sum\limits_{i=1}^N y_i(\Sigma^{-1}x_i - \Sigma^{-1}\mu_1) = 0 \\
> &\Longrightarrow \sum\limits_{i=1}^N y_i\Sigma^{-1}x_i = \sum\limits_{i=1}^N y_i\Sigma^{-1}\mu_1 \\
> &\Longrightarrow \sum\limits_{i=1}^N y_ix_i = \sum\limits_{i=1}^N y_i\mu_1 \\
> &\Longrightarrow \hat{\mu_1} = \dfrac{\sum\limits_{i=1}^N y_ix_i}{\sum\limits_{i=1}^N y_i} = \dfrac{\sum\limits_{i=1}^N y_ix_i}{N_1}
> \end{align*}
> $$
>
> ---
>
> 求$\mu_2$：
> $$
> \begin{align*}
> ② &= \sum\limits_{i=1}^N \log [N(\mu_2,\Sigma)^{1-y_i}] = \sum\limits_{i=1}^N (1-y_i)\log N(\mu_2,\Sigma) \\
> &= \sum\limits_{i=1}^N (1-y_i) \log \{ \dfrac{1}{(2\pi)^{\dfrac{p}{2}} \vert \Sigma \vert ^{\dfrac{1}{2}}} \exp{[-\dfrac{1}{2} (x_i-\mu_2)^T \Sigma^{-1} (x_i-\mu_2)]} \} \\
> &\Longrightarrow \hat{\mu_2} = arg \max_{\mu_2} \sum\limits_{i=1}^N (1-y_i)[-\dfrac{1}{2} (x_i-\mu_2)^T \Sigma^{-1} (x_i-\mu_2)]
> \end{align*}
> $$
>
> $$
> \begin{align*}
> 令\Delta &= -\dfrac{1}{2} \sum\limits_{i=1}^N (1-y_i)[(x_i-\mu_2)^T \Sigma^{-1} (x_i-\mu_2)] \\
> &= -\dfrac{1}{2} \sum\limits_{i=1}^N (1-y_i)[(x_i^T \Sigma^{-1}-\mu_2^T \Sigma^{-1}) (x_i-\mu_2)] \\
> &= -\dfrac{1}{2} \sum\limits_{i=1}^N (1-y_i)(x_i^T\Sigma^{-1}x_i - x_i^T\Sigma^{-1}\mu_2 - \mu_2^T\Sigma^{-1}x_i - \mu_2^T\Sigma^{-1}\mu_2) \\
> &= -\dfrac{1}{2} \sum\limits_{i=1}^N (1-y_i)(x_i^T\Sigma^{-1}x_i - 2\mu_2^T\Sigma^{-1}x_i - \mu_2^T\Sigma^{-1}\mu_2)
> \end{align*}
> $$
>
> $$
> \begin{align*}
> &\dfrac{\partial \Delta}{\partial \mu_2} = -\dfrac{1}{2} \sum\limits_{i=1}^N (1-y_i)(-2\Sigma^{-1}x_i+2\Sigma^{-1}\mu_2) = 0 \\
> &\Longrightarrow \sum\limits_{i=1}^N (1-y_i)x_i = \sum\limits_{i=1}^N (1-y_i)\mu_1 \\
> &\Longrightarrow \hat{\mu_2} = \dfrac{\sum\limits_{i=1}^N (1-y_i)x_i}{\sum\limits_{i=1}^N (1-y_i)} = \dfrac{\sum\limits_{i=1}^N (1-y_i)x_i}{N_2}
> \end{align*}
> $$
>
> ---
>
> 求$\Sigma$：
> $$
> \begin{align*}
> ① + ② &= \sum\limits_{i=1}^N \{ \log [N(\mu_1,\Sigma)^{y_i}] + \log [N(\mu_2,\Sigma)^{1-y_i}] \} \\
> &= \sum\limits_{i=1}^N \{ y_i \log N(\mu_1,\Sigma) + (1-y_i) \log N(\mu_2,\Sigma) \} \\
> &= \sum\limits_{x_i \in c_1}^N \log N(\mu_1,\Sigma) + \sum\limits_{x_i \in c_2}^N \log N(\mu_2,\Sigma)
> \end{align*}
> $$
> $记c_1=\{ x_i \vert y_i=1,i=1,2,...,N \},\vert c_1 \vert = N_1;c_2=\{ x_i \vert y_i=0,i=1,2,...,N \},\vert c_2 \vert = N_2;N_1+N_2=N \\ S_1为c_1类样本方差,S_2为c_2类样本方差$
>
> ---
>
> 补充知识：
>
> 关于迹$(trace)$的运算：
> $$
> \begin{align*}
> & \dfrac{\partial tr(AB)}{\partial A} = B^T \\
> & \dfrac{\partial \vert A \vert}{\partial A} = \vert A \vert A^{-1} \\
> & tr(AB) = tr(BA) \\
> & tr(ABC) = tr(CAB) = tr(CBA) \\
> & \dfrac{\partial tr(S \Sigma^{-1})}{\partial \Sigma} = \dfrac{\partial tr(S \Sigma^{-1})}{\partial \Sigma^{-1}} \dfrac{\partial \Sigma^{-1}}{\partial \Sigma} = S^T*(-1)*\Sigma^{-2} = -S \Sigma^{-2} \quad \textcolor{red}{S^T = S}
> \end{align*}
> $$
> 化简$\sum\limits_{i=1}^N \log N(\mu,\Sigma)$：
> $$
> \begin{align*}
> \sum\limits_{i=1}^N \log N(\mu,\Sigma) &= \sum\limits_{i=1}^N \log \{ \dfrac{1}{(2\pi)^{\dfrac{p}{2}} \vert \Sigma \vert ^{\dfrac{1}{2}}} \exp{[-\dfrac{1}{2} (x_i-\mu)^T \Sigma^{-1} (x_i-\mu)]} \} \\
> &= \sum\limits_{i=1}^N [C + \log \vert \Sigma \vert^{-\dfrac{1}{2}} - \dfrac{1}{2}(x_i-\mu)^T \Sigma^{-1} (x_i-\mu) ] \\
> &= \sum\limits_{i=1}^N [C -\dfrac{1}{2} \log \vert \Sigma \vert - \dfrac{1}{2}(x_i-\mu)^T \Sigma^{-1} (x_i-\mu) ] \quad \textcolor{red}{实数的迹等于它本身}\\
> &= \sum\limits_{i=1}^N [C -\dfrac{1}{2} \log \vert \Sigma \vert ] - \dfrac{1}{2} tr \{ \sum\limits_{i=1}^N [(x_i-\mu)^T \Sigma^{-1} (x_i-\mu)] \} \quad \textcolor{red}{迹运算中矩阵可以交换位置} \\
> &= \sum\limits_{i=1}^N [C -\dfrac{1}{2} \log \vert \Sigma \vert ] - \dfrac{1}{2} tr \{ \sum\limits_{i=1}^N [(x_i-\mu)^T(x_i-\mu) \Sigma^{-1}] \} \quad \textcolor{red}{\dfrac{1}{N} \sum\limits_{i=1}^N [(x_i-\mu)^T(x_i-\mu)] = S(样本方差)} \\
> &= C - \dfrac{N}{2} \log \vert \Sigma \vert - \dfrac{N}{2} tr (S \Sigma^{-1})
> \end{align*}
> $$
>
> ---
>
> 继续求$\Sigma$：
> $$
> \begin{align*}
> ① + ② &= C_1 - \dfrac{N_1}{2} \log \vert \Sigma \vert - \dfrac{N_1}{2} tr (S_1 \Sigma^{-1}) + C_2 - \dfrac{N_2}{2} \log \vert \Sigma \vert - \dfrac{N_2}{2} tr (S_2 \Sigma^{-1}) \\
> &= -\dfrac{1}{2} [N_1 \log \vert\Sigma\vert + N_2 \log \vert\Sigma\vert + N_1 tr(S_1\Sigma^{-1}) + N_2 tr(S_2\Sigma^{-1})] \\
> &= -\dfrac{1}{2} [N \log \vert\Sigma\vert + N_1 tr(S_1\Sigma^{-1}) + N_2 tr(S_2\Sigma^{-1})]
> \end{align*}
> $$
>
> $$
> \begin{align*}
> \dfrac{\partial (① + ②)}{\partial \Sigma} &= -\dfrac{1}{2} (N*\dfrac{1}{\vert\Sigma\vert}*\vert\Sigma\vert*\vert\Sigma\vert^{-1} - N_1S_1\Sigma^{-2} - N_2S_2\Sigma^{-2}) \\
> &= -\dfrac{1}{2} (N\Sigma^{-1} - N_1S_1\Sigma^{-2} - N_2S_2\Sigma^{-2}) = 0 \\
> &\Longrightarrow N\Sigma = N_1S_1 + N_2S_2 \\
> &\Longrightarrow \hat{\Sigma} = \dfrac{1}{N}(N_1S_1 + N_2S_2) \\
> \end{align*}
> $$

## 6、朴素贝叶斯

> 朴素贝叶斯思想：朴素贝叶斯假设(条件独立假设)
>
> 概率图(有向图)：
>
> <img src="F:\笔记\picture\朴素贝叶斯概率图.png" alt="朴素贝叶斯概率图" style="zoom:75%;" />
> $$
> P(x \vert y) = \prod\limits_{j=1}^p P(x_j \vert y) \quad x_j表示每一维
> $$
> 样本数据：$\{(x_i,y_i)\}_{i=1}^N,x_i\in \R^p,y_i\in \{0,1\}$
>
> 优化目标：$\hat{y}=arg\max_{y} P(y\vert x)=arg\max_{y} P(x\vert y)P(y)=arg\max_{y} P(x,y)$
> $$
> \begin{align*}
> & y \left \{ \begin{array}{lr**} 二分类：y \sim Bernoulli\ Dist. \\ 多分类：y \sim Catedorial\ Dist.\end{array} \right. \\
> & x_j \left \{ \begin{array}{lr**} x离散：x_j \sim Catedorial\ Dist. \\ x连续：x_j \sim N(\mu_i,\sigma_j^2) \end{array} \right. 
> \end{align*}
> $$
>
> ---
>
> 补充知识：
>
> | 实验次数：1次 | 实验次数：n次  |
> | :-----------: | :------------: |
> |  $Bernoulli$  |  $Binormial$   |
> | $Categorical$ | $Multinormail$ |
>
> 



